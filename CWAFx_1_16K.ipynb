{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75394eb-e7ac-49d6-81f1-ff4242d3b5b3",
   "metadata": {},
   "source": [
    "### Dataset: https://www.idmt.fraunhofer.de/en/publications/datasets/audio_effects.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be1d7fb-b223-4416-91cc-649bb19ae97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, Dense, Activation, Concatenate, TimeDistributed, Lambda, Reshape\n",
    "from keras.layers import Multiply, Add, UpSampling1D, MaxPooling1D, Bidirectional, LSTM, GlobalAvgPool1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python.keras.utils import conv_utils\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "\n",
    "import Utils as utils\n",
    "from keras.utils import Sequence\n",
    "\n",
    "#from Layers import Generator\n",
    "import Models\n",
    "\n",
    "from Layers import Conv1D_local, Dense_local, SAAF, Conv1D_tied, Slice\n",
    "\n",
    "import random\n",
    "import librosa\n",
    "import scipy\n",
    "import soundfile as sf\n",
    "import json\n",
    "\n",
    "random.seed(4264523625)\n",
    "\n",
    "import brian2\n",
    "from brian2hears import erbspace, Gammatone, Sound\n",
    "from brian2 import Hz\n",
    "from scipy.signal import hilbert\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics.pairwise import paired_distances\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "import os\n",
    "import wave\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "from scipy.io import wavfile\n",
    "import resampy\n",
    "import ctypes\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, message=\".*hann from 'scipy.signal' is deprecated.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c074da-4110-40c2-96f8-5566bffd43ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2147483648"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prevent sleep\n",
    "ES_CONTINUOUS = 0x80000000\n",
    "ES_SYSTEM_REQUIRED = 0x00000001\n",
    "ctypes.windll.kernel32.SetThreadExecutionState(ES_CONTINUOUS | ES_SYSTEM_REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802ee9c7-2385-426d-9dc7-bc47939f5302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a6af67-f126-437e-8abd-690e367687b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kSR = 16000\n",
    "kContext = 4\n",
    "epoch =  4\n",
    "filters = 128\n",
    "kernelSize = 64\n",
    "learningRate = 0.0001\n",
    "winLength = 4096\n",
    "modelsPath = \"./Models/\"\n",
    "monitorLoss =  \"val_loss\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002111a4-b31c-476b-bf4d-cb5b063b73ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BooleanMask(x):\n",
    "    output = tf.cast(tf.greater_equal(x[0], x[1]), dtype=tf.float32)\n",
    "    output = tf.multiply(output, x[1])\n",
    "    return output\n",
    "\n",
    "def toPermuteDimensions(x):\n",
    "    return K.permute_dimensions(x, (0, 2, 1))\n",
    "\n",
    "def absolute_activation(x):\n",
    "    return tf.abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b558b1d2-73b2-45e8-a8fb-34c11979e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(x, x_len, win_length, hop_length, windowing=True, rate=1):\n",
    "    x = x.reshape(x.shape[0], x.shape[1]).T\n",
    "    \n",
    "    if windowing:\n",
    "        window = scipy.signal.hann(win_length, sym=False)\n",
    "        #window = scipy.signal.get_window(('hann', False), win_length)\n",
    "        rate = rate * hop_length / win_length\n",
    "    else:\n",
    "        window = 1\n",
    "        rate = 1\n",
    "    n_frames = x_len / hop_length\n",
    "    expected_signal_len = int(win_length + hop_length * (n_frames))\n",
    "    y = np.zeros(expected_signal_len)\n",
    "    for i in range(int(n_frames)):\n",
    "        sample = i * hop_length\n",
    "        w = x[:, i]\n",
    "        y[sample:(sample + win_length)] = y[sample:(sample + win_length)] + w * window\n",
    "    y = y[int(win_length // 2):-int(win_length // 2)]\n",
    "    return np.float32(y * rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccea95b9-420f-4d83-bca9-b2857d8879b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kGfmin, kGfmax, kGbands = 26, 6950, 12\n",
    "kMfmin, kMfmax, kMbands = 0.5, 100, 12\n",
    "kEsr = 400\n",
    "\n",
    "cfG = erbspace(kGfmin*Hz, kGfmax*Hz, kGbands)\n",
    "cfM = erbspace(kMfmin*Hz, kMfmax*Hz, kMbands)\n",
    "\n",
    "kEfmin = cfM[:-1]\n",
    "kEfmax = cfM[1:]\n",
    "\n",
    "def getGammatone(x, fmin, fmax, bands, sr):\n",
    "    cf = erbspace(fmin*Hz, fmax*Hz, bands)\n",
    "    gfb = Gammatone(Sound(x, samplerate=sr*Hz), cf)\n",
    "    gamma = gfb.process()\n",
    "    return gamma\n",
    "\n",
    "def getFFT(x):\n",
    "    n = len(x) # length of the signal\n",
    "    x = x*scipy.signal.hann(n, sym=False)\n",
    "    XD = np.fft.fft(x)/n # fft computing and normalization\n",
    "    XD = XD[range(n//2)]\n",
    "    \n",
    "    return XD\n",
    "\n",
    "def getModulation(x, fmin, fmax, bands, sr):\n",
    "    cf = erbspace(fmin*Hz, fmax*Hz, bands)\n",
    "    m = []\n",
    "    for i in range(x.shape[1]):\n",
    "        gfb = Gammatone(Sound(x[:,i], samplerate=sr*Hz), cf)\n",
    "        m.append(gfb.process())\n",
    "    return np.asarray(m)\n",
    "\n",
    "def getEnvelope(x, power = True, downsample = None):\n",
    "    \n",
    "    envs = []\n",
    "    for i in range(x.shape[1]):\n",
    "        analytic_signal = hilbert(x[:,i])\n",
    "        amplitude_envelope = (np.abs(analytic_signal))\n",
    "        if power:\n",
    "            amplitude_envelope = np.power(amplitude_envelope, 0.3)\n",
    "        if downsample:\n",
    "            amplitude_envelope = scipy.signal.resample(amplitude_envelope,\n",
    "                                                       len(amplitude_envelope)//(downsample[0]//downsample[1]))\n",
    "        envs.append(amplitude_envelope)\n",
    "    envs = np.asarray(envs)\n",
    "    \n",
    "    return envs.T\n",
    "\n",
    "def getMarginal(x):\n",
    "    \n",
    "    stats = OrderedDict()\n",
    "    marginal = scipy.stats.describe(x)\n",
    "    stats['mean'] = marginal[2]\n",
    "    stats['var'] = marginal[3]/(marginal[2]**2)\n",
    "    stats['skew'] = marginal[4]\n",
    "    stats['kurtosis'] = marginal[5]\n",
    "    return stats\n",
    "\n",
    "def plotSubBands(X, sr):\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    t = np.linspace(0, len(X[:,0])/sr, num=len(X[:,0]))\n",
    "    for i in range(X.shape[1]):\n",
    "        plf.plot(t, X[:,i])\n",
    "        \n",
    "\n",
    "def getModulationPower(x_m, x_e):\n",
    "    m = []\n",
    "    for i in range(kGbands):\n",
    "        a = scipy.stats.describe(x_m[i]**2)\n",
    "        m.append(a[2])\n",
    "    return np.asarray(m) \n",
    "\n",
    "def getModulationSpectrum(x):\n",
    "    x_mD = []\n",
    "    for j in range(x.shape[0]):\n",
    "        ffts = []\n",
    "        for i in range(x.shape[2]):\n",
    "            XD = getFFT(x[j,:,i])\n",
    "            ffts.append(abs(XD))\n",
    "        x_mD.append(ffts)\n",
    "    x_mD = np.asarray(x_mD)\n",
    "    return x_mD\n",
    "\n",
    "def getModulationSpectrumEnergy(x, f1, f2, sr, normalizeDC = True):\n",
    "    \n",
    "    #returns modulation spectrum and modulation energy within specifc frequency bands\n",
    "    energy = np.mean(x, axis = 1)**2\n",
    "    if normalizeDC:\n",
    "        energy = np.mean(energy, axis = 0)/np.mean(energy, axis = 0)[0]\n",
    "    else:\n",
    "        energy = np.mean(energy, axis = 0)\n",
    "    \n",
    "    energyBands = []\n",
    "    for fmin, fmax in zip(f1, f2):\n",
    "        \n",
    "        bin1 = int(np.round(energy.shape[0]*fmin/(sr/2)))\n",
    "        bin2 = int(np.round(energy.shape[0]*fmax/(sr/2)))\n",
    "        energyBands.append(np.sum(energy[bin1:bin2+1]))\n",
    "    \n",
    "    modulationspectrum = np.mean(x, axis = 1)\n",
    "    modulationspectrum = np.mean(modulationspectrum, axis = 0)\n",
    "        \n",
    "    return modulationspectrum, np.asarray(energyBands)\n",
    "\n",
    "def getMeanLogModulationSpectrum(x):\n",
    "    \n",
    "    x = np.mean(x, axis=0)\n",
    "    x = np.mean(x, axis=0)\n",
    "    x = np.log(x + 1e-10)\n",
    "    return x\n",
    "\n",
    "def getMP(audio, kLen):\n",
    "\n",
    "    x_modulationEnergyBands = []\n",
    "\n",
    "   \n",
    "    x_g = getGammatone(audio[:kLen], kGfmin, kGfmax, kGbands, kSR)\n",
    "    x_ge = getEnvelope(x_g, downsample = ((kSR, kEsr)), power = False)\n",
    "    x_gem = getModulation(x_ge, kMfmin, kMfmax, kMbands, kEsr)\n",
    "    x_ge_stats = getMarginal(x_ge)\n",
    "    m_x = getModulationPower(x_gem, x_ge_stats)\n",
    "    x_gemD = getModulationSpectrum(x_gem)\n",
    "    x_Em, x_Ebm = getModulationSpectrumEnergy(x_gemD, kEfmin, kEfmax, kEsr, normalizeDC = True)\n",
    "    x_em_stats = getMarginal(x_Em[1:])\n",
    "    x_gemD_meanlog = getMeanLogModulationSpectrum(x_gemD)\n",
    "    \n",
    "    \n",
    "    return x_gemD_meanlog, x_Ebm, x_em_stats.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62027043-3ba5-4519-be9c-1e93b21198ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Array Shape:  (104, 192000)\n",
      "Test Array Shape (104, 192000)\n",
      "Output Array Shape (104, 192000)\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"C:/Users/yavuz/Musiccodes/MyCAFModels/data/\")\n",
    "\n",
    "nofx_2_array = np.load(\"grandhall_stereo_data_right.npy\")\n",
    "print(\"Training Array Shape: \", nofx_2_array.shape)\n",
    "\n",
    "nofx_2_array_test = np.load(\"grandhall_stereo_data_right.npy\")\n",
    "print(\"Test Array Shape\", nofx_2_array_test.shape)\n",
    "\n",
    "reverb_2_array = np.load(\"grandhall_atmos_data_center.npy\")\n",
    "print(\"Output Array Shape\", reverb_2_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836bec39-3f04-4c24-9670-2ef5707c23f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(reverb_2_array[5] - nofx_2_array[5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea1aa4e-ae94-48b5-81c0-f30281a9c1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "(104, 64000) (104, 64000) (104, 64000)\n"
     ]
    }
   ],
   "source": [
    "nofx_2_array_16 = np.zeros((104, 64000), dtype=np.float32)\n",
    "reverb_2_array_16 = np.zeros((104, 64000), dtype=np.float32)\n",
    "nofx_2_array_test_16 = np.zeros((104, 64000), dtype=np.float32)\n",
    "\n",
    "for i in range(104):\n",
    "    if (i%20==0):\n",
    "        print(i)\n",
    "    nofx_2_array_test_16[i,:] = resampy.resample(nofx_2_array_test[i,:], sr_orig=48000, sr_new=16000)\n",
    "    nofx_2_array_16[i,:] = resampy.resample(nofx_2_array[i,:], sr_orig=48000, sr_new=16000)\n",
    "    reverb_2_array_16[i,:] = resampy.resample(reverb_2_array[i,:], sr_orig=48000, sr_new=16000)\n",
    "    \n",
    "print(nofx_2_array_test_16.shape, nofx_2_array_16.shape,reverb_2_array_16.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d7668-5f9d-45e0-99fc-1884cf199414",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(reverb_2_array_16[5] - nofx_2_array_16[5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e6a15fc-06ab-4182-b79c-e6101cda9df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104, 64000) (104, 64000) (104, 64000)\n"
     ]
    }
   ],
   "source": [
    "nofx_2_array_test = nofx_2_array_test_16\n",
    "nofx_2_array = nofx_2_array_16\n",
    "reverb_2_array = reverb_2_array_16\n",
    "\n",
    "print(nofx_2_array_test.shape, nofx_2_array.shape, reverb_2_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc0b6c4-e29e-43d2-8b03-645dfbace517",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(reverb_2_array[5] - nofx_2_array[5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "053b0e38-b0db-4b16-8521-727964e76c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (104, 64000)\n",
      "Output (104, 64000)\n",
      "Test (104, 64000)\n",
      "Length in samples 64000\n"
     ]
    }
   ],
   "source": [
    "data_length_include = 104\n",
    "\n",
    "nofx_2_array = nofx_2_array[0:data_length_include,:]\n",
    "print(\"Train\", nofx_2_array.shape)\n",
    "\n",
    "reverb_2_array = reverb_2_array[0:data_length_include,:]\n",
    "print(\"Output\", reverb_2_array.shape)\n",
    "\n",
    "nofx_2_array_test = nofx_2_array_test[0:data_length_include,:]\n",
    "print(\"Test\", nofx_2_array_test.shape)\n",
    "\n",
    "length_in_samples = nofx_2_array.shape[1]\n",
    "\n",
    "print(\"Length in samples\", length_in_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3199d8a1-065c-45e3-a397-abe22de2dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nofx_2_array_reshaped = nofx_2_array.reshape(data_length_include, 32001, 1)\n",
    "#reverb_2_array_reshaped = reverb_2_array.reshape(data_length_include, 32001, 1)\n",
    "\n",
    "nofx_2_array_reshaped = nofx_2_array.reshape(data_length_include, length_in_samples, 1)\n",
    "nofx_2_array_test_reshaped = nofx_2_array_test.reshape(data_length_include, length_in_samples, 1)\n",
    "reverb_2_array_reshaped = reverb_2_array.reshape(data_length_include, length_in_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaaf7f2-c201-40ae-9fd9-0b57849a20a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_x = 12\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(nofx_2_array[id_x]);\n",
    "plt.title(\"Train\")\n",
    "Audio(data=nofx_2_array[id_x], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fed81d-d855-4175-a858-49d36fabbe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(reverb_2_array[id_x]);\n",
    "plt.title(\"Output\")\n",
    "Audio(data=reverb_2_array[id_x], rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9051679f-c3eb-4674-9717-eaa3562a2af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_x = 12\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(reverb_2_array[id_x] - nofx_2_array[id_x]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65c6362f-66b1-48b5-96df-93f2676fca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, win_length, hop_length, win = False):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.batch_size = int(self.x.shape[1] / self.hop_length) + 1\n",
    "        self.win = win\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_x = np.zeros((self.batch_size, self.win_length, 1))\n",
    "\n",
    "        batch_y = np.zeros((self.batch_size, self.win_length, 1))\n",
    "\n",
    "\n",
    "        x_w = self.x[idx].reshape(len(self.x[idx]))\n",
    "        y_w = self.y[idx].reshape(len(self.y[idx]))\n",
    "\n",
    "\n",
    "        x_w = utils.slicing(x_w, self.win_length, self.hop_length, windowing = self.win)\n",
    "        y_w = utils.slicing(y_w, self.win_length, self.hop_length, windowing = self.win)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "\n",
    "            batch_x[i] = x_w[i].reshape(self.win_length ,1)\n",
    "            batch_y[i] = y_w[i].reshape(self.win_length ,1)\n",
    "\n",
    "\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb4b6989-6bcc-44bc-8d61-4f4de5c39c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples 83\n",
      "Xtrain.shape (83, 63999, 1) (83, 63999, 1) (21, 63999, 1) (21, 63999, 1)\n",
      "Xtrain.shape after CropandPad (83, 72191, 1) (83, 72191, 1) (21, 72191, 1) (21, 72191, 1)\n"
     ]
    }
   ],
   "source": [
    "train_samples = int(len(nofx_2_array)*0.8)\n",
    "print(\"Train samples\", train_samples)\n",
    "\n",
    "\n",
    "Xtrain = nofx_2_array_reshaped[0:train_samples,1:,:]\n",
    "#Ytrain = chorus_array_reshaped[0:62,1:,:]\n",
    "Ytrain = reverb_2_array_reshaped[0:train_samples,1:,:]\n",
    "Xval = nofx_2_array_reshaped[train_samples:,1:,:]\n",
    "#Yval = chorus_array_reshaped[62:,1:,:]\n",
    "Yval = reverb_2_array_reshaped[train_samples:,1:,:]\n",
    "\n",
    "print(\"Xtrain.shape\", Xtrain.shape, Ytrain.shape, Xval.shape, Yval.shape)\n",
    "\n",
    "# since the samples are 2 secs long, we zero pad kContext*hop_size samples at the end of the recording. This for the 4 \n",
    "# subsequent frames in the Leslie modeling tasks.\n",
    "Xtrain = utils.cropAndPad(Xtrain, crop = 0, pad = kContext*winLength//2)\n",
    "Ytrain = utils.cropAndPad(Ytrain, crop = 0, pad = kContext*winLength//2)\n",
    "Xval = utils.cropAndPad(Xval, crop = 0, pad = kContext*winLength//2)\n",
    "Yval = utils.cropAndPad(Yval, crop = 0, pad = kContext*winLength//2)\n",
    "\n",
    "print(\"Xtrain.shape after CropandPad\", Xtrain.shape, Ytrain.shape, Xval.shape, Yval.shape)\n",
    "\n",
    "Xtrain_pre = np.vstack((Xtrain, Ytrain))\n",
    "Xval_pre = np.vstack((Xval, Yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3794a396-4e83-4f96-ae5d-073248b5d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGen_pre = Generator(Xtrain_pre, Xtrain_pre, winLength, winLength//2)\n",
    "valGen_pre = Generator(Xval_pre, Xval_pre, winLength, winLength//2)   \n",
    "\n",
    "trainGen = Generator(Xtrain, Ytrain, winLength, winLength//2)\n",
    "valGen = Generator(Xval, Yval, winLength, winLength//2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68c20d58-0884-41ba-83c0-3beb203cac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"Model_CWAFx\"\n",
    "\n",
    "earlyStopping_pre = Models.EarlyStopping(monitor='loss',\n",
    "                                          min_delta=0,\n",
    "                                          patience=25,\n",
    "                                          verbose=1,\n",
    "                                          mode='auto',\n",
    "                                          baseline=None, \n",
    "                                          restore_best_weights=False)\n",
    "\n",
    "\n",
    "checkpointer_pre = Models.ModelCheckpoint(filepath=modelsPath + modelName + \"_chk.weights.h5\",\n",
    "                                           monitor=monitorLoss,\n",
    "                                           verbose=1,\n",
    "                                           save_best_only=True,\n",
    "                                           save_weights_only=True)  \n",
    "\n",
    "earlyStopping = Models.EarlyStopping(monitor='loss',\n",
    "                                          min_delta=0,\n",
    "                                          patience=25,\n",
    "                                          verbose=1,\n",
    "                                          mode='auto',\n",
    "                                          baseline=None, restore_best_weights=False)\n",
    "\n",
    "checkpointer = Models.ModelCheckpoint(filepath=modelsPath + modelName +\".weights.h5\",\n",
    "                                           monitor=monitorLoss,\n",
    "                                           verbose=1,\n",
    "                                           save_best_only=True,\n",
    "                                           save_weights_only=True)  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "401e1e48-6176-418b-8b6e-5ea247345891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\yavuz\\\\Musiccodes\\\\MyCAFModels\\\\data'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcec38a-0f34-4909-aa06-83526980bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Xtrain Shape\", Xtrain.shape, \"Ytrain Shape\", Ytrain.shape, \"Xval Shape\", Xval.shape, \"Yval Shape\", Yval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab7fe6fb-a0a9-4cfe-9968-39e5ec62853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrainingModel(win_length, filters, kernel_size_1, learning_rate):\n",
    "\n",
    "    x = Input(shape=(win_length, 1), name='input')\n",
    "    print(\"PRETRAINING\")\n",
    "    #print(x)\n",
    "    conv = Conv1D(filters, kernel_size_1, strides=1, padding='same',\n",
    "                  kernel_initializer='lecun_uniform',\n",
    "                  input_shape=(win_length, 1), name='conv')\n",
    "\n",
    "    conv_smoothing = Conv1D_local(filters, kernel_size_1 * 2, strides=1, padding='same',\n",
    "                                  kernel_initializer='lecun_uniform', name='conv_smoothing')\n",
    "\n",
    "    deconv = Conv1D_tied(1, kernel_size_1, conv, padding='same', name='deconv')\n",
    "\n",
    "    X = conv(x)\n",
    "    #print(\"X shape\", X.shape)\n",
    "    #X_abs = Activation(K.abs, name='conv_activation')(X)\n",
    "    #X_abs = Lambda(lambda x: K.abs(x), name='conv_activation')(X)\n",
    "    #X_abs = Activation(absolute_activation, name='conv_activation')(X)\n",
    "    #X_abs = Lambda(lambda x: K.abs(x), name='conv_activation')(X)\n",
    "\n",
    "    #X_abs = Lambda(lambda x: K.abs(x), name='conv_activation', output_shape=lambda s:s)(X)\n",
    "    #X_abs = Lambda(lambda x: Activation(x), name='conv_activation', output_shape=lambda s:s)(X)\n",
    "    X_abs = Lambda(lambda x: tf.abs(x), name='conv_activation')(X)\n",
    "\n",
    "    #X_abs = Activation(absolute_activation, name='conv_activation')(X)\n",
    "    \n",
    "    M = conv_smoothing(X_abs)\n",
    "    M = Activation('softplus', name='conv_smoothing_activation')(M)\n",
    "    print(\"M Shape\", M.shape)\n",
    "\n",
    "    P = X\n",
    "    Z = MaxPooling1D(pool_size=win_length // 64, name='max_pooling')(M)\n",
    "    M_ = UpSampling1D(size=win_length // 64, name='up_sampling_naive')(Z)\n",
    "\n",
    "    #print(\"M_ Shape\", M_.shape)\n",
    "    #M_ = Lambda((BooleanMask), name='boolean_mask')([M, M_])\n",
    "\n",
    "    M_ = Lambda(BooleanMask, name='boolean_mask', output_shape=lambda s: s[0])([M, M_])\n",
    "    \n",
    "\n",
    "    Y = Multiply(name='phase_unpool_multiplication')([P, M_])\n",
    "    Y = deconv(Y)\n",
    "\n",
    "    model = Model(inputs=[x], outputs=[Y])\n",
    "\n",
    "    model.compile(loss={'deconv': 'mae'},\n",
    "                  loss_weights={'deconv': 1.0},\n",
    "                  optimizer=Adam(learning_rate=learning_rate))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e306a-3b7b-4a1e-919c-9ac380347cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrainingModel(win_length, filters, kernel_size_1, learning_rate):\n",
    "\n",
    "    x = Input(shape=(win_length, 1), name='input')\n",
    "    conv = Conv1D(filters, kernel_size_1, strides=1, padding='same',\n",
    "                  kernel_initializer='lecun_uniform',\n",
    "                  input_shape=(win_length, 1), name='conv')\n",
    "\n",
    "    conv_smoothing = Conv1D_local(filters, kernel_size_1 * 2, strides=1, padding='same',\n",
    "                                  kernel_initializer='lecun_uniform', name='conv_smoothing')\n",
    "\n",
    "    deconv = Conv1D_tied(1, kernel_size_1, conv, padding='same', name='deconv')\n",
    "\n",
    "    X = conv(x)\n",
    "\n",
    "    X_abs = Lambda(lambda x: tf.abs(x), name='conv_activation')(X)\n",
    "\n",
    "    M = conv_smoothing(X_abs)\n",
    "    M = Activation('softplus', name='conv_smoothing_activation')(M)\n",
    "    print(\"M Shape\", M.shape)\n",
    "\n",
    "    P = X\n",
    "    Z = MaxPooling1D(pool_size=win_length // 64, name='max_pooling')(M)\n",
    "    M_ = UpSampling1D(size=win_length // 64, name='up_sampling_naive')(Z)\n",
    "\n",
    "\n",
    "    M_ = Lambda(BooleanMask, name='boolean_mask', output_shape=lambda s: s[0])([M, M_])\n",
    "    \n",
    "\n",
    "    Y = Multiply(name='phase_unpool_multiplication')([P, M_])\n",
    "    Y = deconv(Y)\n",
    "\n",
    "    model = Model(inputs=[x], outputs=[Y])\n",
    "\n",
    "    model.compile(loss={'deconv': 'mae'},\n",
    "                  loss_weights={'deconv': 1.0},\n",
    "                  optimizer=Adam(learning_rate=learning_rate))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0054b392-190e-4cc5-935b-bbed81f877f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    C:\\Users\\yavuz\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      " [py.warnings]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRETRAINING\n",
      "WARNING:tensorflow:From C:\\Users\\yavuz\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "M Shape (None, 4096, 128)\n"
     ]
    }
   ],
   "source": [
    "model_CWAFx_pretraining = pretrainingModel(winLength,\n",
    "                                    filters, \n",
    "                                    kernelSize, \n",
    "                                    learningRate);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a65f7-ba1a-4982-b0e2-42c7c76ac02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CWAFx_pretraining = pretrainingModel(winLength,\n",
    "                                    filters, \n",
    "                                    kernelSize, \n",
    "                                    learningRate);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f61ae445-51ab-4e56-9443-018e7e483c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    C:\\Users\\yavuz\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      " [py.warnings]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    C:\\Users\\yavuz\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['input']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n",
      " [py.warnings]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0046"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    C:\\Users\\yavuz\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      " [py.warnings]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.00226, saving model to ./Models/Model_CWAFx_chk.weights.h5\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 3s/step - loss: 0.0046 - val_loss: 0.0023\n",
      "Epoch 2/2\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.0035\n",
      "Epoch 2: val_loss did not improve from 0.00226\n",
      "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 3s/step - loss: 0.0035 - val_loss: 0.0050\n"
     ]
    }
   ],
   "source": [
    "model_CWAFx_pretraining.fit(trainGen_pre,\n",
    "                           steps_per_epoch=None,\n",
    "                           epochs=2,\n",
    "                           verbose=1,\n",
    "                           callbacks = [checkpointer_pre, earlyStopping_pre],\n",
    "                           validation_data = valGen_pre,\n",
    "                           validation_steps=len(Xval),\n",
    "                           shuffle=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b73008f-a4f4-4115-bbcd-35309debcaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(trainGen_pre,\n",
    "          steps_per_epoch=None,\n",
    "                           epochs=2,\n",
    "                           verbose=2,\n",
    "                           callbacks = [checkpointer_pre, earlyStopping_pre],\n",
    "                           validation_data = valGen_pre,\n",
    "                           validation_steps=len(Xval),\n",
    "                           shuffle=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d4755-9cc9-4e2a-8d68-2117b68d2027",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CWAFx_pretraining.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df7a5e94-a5c0-43bf-855b-ae610c8fec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorContext(Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, context, win_length, hop_length, win = False, win_input = None):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.win_length = win_length\n",
    "        self.hop_length = hop_length\n",
    "        self.batch_size = int(self.x.shape[1] / self.hop_length) + 1\n",
    "        self.win_output = win\n",
    "        if win_input == None:\n",
    "            self.win_input = win\n",
    "        else:\n",
    "            self.win_input = win_input\n",
    "        self.context = context\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        batch_x = []\n",
    "        for i in range(self.context*2+1):\n",
    "            batch_x.append(np.zeros((self.batch_size, self.win_length, 1)))\n",
    "        batch_y = np.zeros((self.batch_size, self.win_length, 1))\n",
    "        \n",
    "        \n",
    "        x_w = self.x[idx].reshape(len(self.x[idx]))\n",
    "        y_w = self.y[idx].reshape(len(self.y[idx]))\n",
    "\n",
    "        \n",
    "        x_w = utils.slicing(x_w, self.win_length, self.hop_length, windowing = self.win_input)\n",
    "\n",
    "        x_w = np.pad(x_w, ((self.context, self.context),(0, 0)), 'constant', constant_values=(0))\n",
    "        a = []\n",
    "        for i in range(x_w.shape[0]):\n",
    "            a.append(x_w[i:i+self.context*2+1])\n",
    "        del a[-self.context*2:]\n",
    "        a = np.asarray(a)\n",
    "       \n",
    "        y_w = utils.slicing(y_w, self.win_length, self.hop_length, windowing = self.win_output)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            \n",
    "            for j in range(self.context*2+1):\n",
    "                batch_x[j][i] = a[:,j,:][i].reshape(self.win_length,1)\n",
    "                       \n",
    "            batch_y[i] = y_w[i].reshape(self.win_length,1) \n",
    "            \n",
    "        batch_x = np.swapaxes(np.asarray(batch_x), 0, 1)\n",
    "        \n",
    "        return batch_x, batch_y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b3c8543-6445-4287-94f5-3594d2a70d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_block(x, num_features, weight_decay=0., amplifying_ratio=16, idx = 1):\n",
    "    x = Multiply(name='dnn-saaf-se_%s'%idx)([x, se_fn(x, amplifying_ratio, idx)])\n",
    "    return x\n",
    "def se_fn(x, amplifying_ratio, idx):\n",
    "    #num_features = x.shape[-1].value\n",
    "    num_features = x.shape[-1]\n",
    "    x = Activation(K.abs)(x)\n",
    "    x = GlobalAvgPool1D()(x)\n",
    "    x = Reshape((1, num_features))(x)\n",
    "    x = Dense(num_features * amplifying_ratio, activation='relu', kernel_initializer='glorot_uniform',\n",
    "              name='se_dense1_%s'%idx)(x)\n",
    "    x = Dense(num_features, activation='sigmoid', kernel_initializer='glorot_uniform',\n",
    "              name='se_dense2_%s'%idx)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7886e5-4279-43e3-aa06-6d0c38413520",
   "metadata": {},
   "source": [
    "## CWAFx and Wavenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02bd2c2b-1bec-4b2a-b041-599df4522207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilated_residual_block(data_x, res_block_i, layer_i, dilation, stack_i, config,\n",
    "                           num_residual_blocks, input_length, samples_of_interest_indices,\n",
    "                           padded_target_field_length, context=True):\n",
    "    \n",
    "    \n",
    "\n",
    "    original_x = data_x\n",
    "    bias = True\n",
    "\n",
    "    # Data sub-block\n",
    "    data_out = keras.layers.Conv1D(2 * config['model']['filters']['depths']['res'],\n",
    "                                                config['model']['filters']['lengths']['res'],\n",
    "                                                dilation_rate=dilation, padding='same',\n",
    "                                                use_bias=bias,\n",
    "                                                name='res_%d_dilated_conv_d%d_s%d' % (\n",
    "                                                res_block_i, dilation, stack_i),\n",
    "                                                activation=None)(data_x)\n",
    "    \n",
    "    data_out_1 = Slice(\n",
    "            (Ellipsis, slice(0, config['model']['filters']['depths']['res'])),\n",
    "            (input_length, config['model']['filters']['depths']['res']),\n",
    "            name='res_%d_data_slice_1_d%d_s%d' % (num_residual_blocks, dilation, stack_i))(data_out)\n",
    "    \n",
    "    data_out_2 = Slice(\n",
    "            (Ellipsis, slice(config['model']['filters']['depths']['res'],\n",
    "                             2 * config['model']['filters']['depths']['res'])),\n",
    "            (input_length, config['model']['filters']['depths']['res']),\n",
    "            name='res_%d_data_slice_2_d%d_s%d' % (num_residual_blocks, dilation, stack_i))(data_out)\n",
    "   \n",
    "   \n",
    "    \n",
    "    tanh_out = keras.layers.Activation('tanh')(data_out_1)\n",
    "    sigm_out = keras.layers.Activation('sigmoid')(data_out_2)\n",
    "    \n",
    "    data_x = keras.layers.Multiply(name='res_%d_gated_activation_%d_s%d'\n",
    "                                   % (res_block_i, layer_i, stack_i))([tanh_out, sigm_out])\n",
    "\n",
    "    data_x = keras.layers.Conv1D(config['model']['filters']['depths']['res']\n",
    "                                 + config['model']['filters']['depths']['skip'],\n",
    "                                1,\n",
    "                                padding='same',\n",
    "                                use_bias = bias,\n",
    "                                name='res_%d_conv_d%d_s%d' % (\n",
    "                                res_block_i, dilation, stack_i))(data_x)\n",
    "    \n",
    "    res_x = Slice((Ellipsis, slice(0, config['model']['filters']['depths']['res'])),\n",
    "                             (input_length, config['model']['filters']['depths']['res']),\n",
    "                         name='res_%d_data_slice_3_d%d_s%d' % (res_block_i, dilation, stack_i))(data_x)\n",
    "    \n",
    "    skip_x = Slice((Ellipsis, slice(config['model']['filters']['depths']['res'],\n",
    "                                               config['model']['filters']['depths']['res'] +\n",
    "                                               config['model']['filters']['depths']['skip'])),\n",
    "                              (input_length, config['model']['filters']['depths']['skip']),\n",
    "                          name='res_%d_data_slice_4_d%d_s%d' % (res_block_i, dilation, stack_i))(data_x)\n",
    "    \n",
    "    if context == False:\n",
    "        samples_of_interest_indices[0] = 0\n",
    "        samples_of_interest_indices[-1] = config['model']['input_length']#k['win_length']\n",
    "    \n",
    "    skip_x = Slice((slice(samples_of_interest_indices[0], samples_of_interest_indices[-1], 1),\n",
    "                               Ellipsis),\n",
    "                   (padded_target_field_length, config['model']['filters']['depths']['skip']),\n",
    "                   name='res_%d_keep_samples_of_interest_d%d_s%d' % (res_block_i, dilation, stack_i))(skip_x)\n",
    "    \n",
    "    res_x = keras.layers.Add()([original_x, res_x])\n",
    "    \n",
    "\n",
    "    return res_x, skip_x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bae9e1ec-03c8-4971-ae13-3c36196b21f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavenet(data_input, config, contextFrames=0, output_channels=1, context=True):\n",
    "\n",
    "    \n",
    "\n",
    "    num_residual_blocks = len(config['model']['dilations']) * config['model']['num_stacks']\n",
    "    \n",
    "    input_length = config['model']['input_length']\n",
    "    if input_length == None:\n",
    "        input_length = config['model']['target_field_length']*(contextFrames + 1)\n",
    "    \n",
    "    target_field_length = config['model']['target_field_length']\n",
    "    half_target_field_length = target_field_length // 2\n",
    "    target_padding = config['model']['target_padding']\n",
    "    target_sample_index = int(np.floor(input_length / 2.0))\n",
    "    samples_of_interest_indices = range(target_sample_index - half_target_field_length - target_padding,\n",
    "                                        target_sample_index + half_target_field_length + target_padding + 1)\n",
    "    padded_target_field_length = target_field_length + 2 * target_padding\n",
    "   \n",
    "    \n",
    "    data_out = keras.layers.Conv1D(config['model']['filters']['depths']['res'],\n",
    "                                                  config['model']['filters']['lengths']['res'], padding='same',\n",
    "                                                  use_bias=True, name='initial_causal_conv')(data_input)\n",
    "    \n",
    "    skip_connections = []\n",
    "    res_block_i = 0\n",
    "    for stack_i in range(config['model']['num_stacks']):\n",
    "        layer_in_stack = 0\n",
    "        for dilation in config['model']['dilations']:\n",
    "            res_block_i += 1\n",
    "            data_out, skip_out = dilated_residual_block(data_out,\n",
    "                                                        res_block_i,\n",
    "                                                        layer_in_stack,\n",
    "                                                        dilation,\n",
    "                                                        stack_i,\n",
    "                                                        config, \n",
    "                                                        num_residual_blocks, \n",
    "                                                        input_length, \n",
    "                                                        samples_of_interest_indices, \n",
    "                                                        padded_target_field_length,\n",
    "                                                       context=context)\n",
    "            if skip_out is not None:\n",
    "                skip_connections.append(skip_out)\n",
    "            layer_in_stack += 1\n",
    "\n",
    "    skip_connections = keras.layers.Lambda(lambda inputs: tf.convert_to_tensor(inputs))(skip_connections)        \n",
    "\n",
    "    skip_connections = keras.layers.Lambda(lambda inputs: tf.keras.backend.sum(inputs,\n",
    "                                                                               axis=0,\n",
    "                                                                               keepdims=False))(skip_connections)   \n",
    "\n",
    "    data_out = keras.layers.Activation('relu')(skip_connections)\n",
    "\n",
    "    data_out = keras.layers.Conv1D(config['model']['filters']['depths']['final'][0],\n",
    "                                          config['model']['filters']['lengths']['final'][0], padding='same',\n",
    "                                                  use_bias=True, name='penultimate_conv_1d')(data_out)\n",
    "\n",
    "    \n",
    "    data_out = keras.layers.Activation('relu')(data_out)\n",
    "\n",
    "    data_out = keras.layers.Conv1D(config['model']['filters']['depths']['final'][1],\n",
    "                                          config['model']['filters']['lengths']['final'][1], padding='same',\n",
    "                                                  use_bias=True, name='final_conv_1d')(data_out)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return data_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "342e4cb1-8223-4e0e-afd5-3eefe7cb2693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WaveNet(learning_rate, wavenetConfig):\n",
    "    \n",
    "    data_input = Input(shape=(wavenetConfig['model']['input_length'], 1), name='data_input')\n",
    "\n",
    "    \n",
    "    data_out = wavenet(data_input, wavenetConfig)\n",
    "    \n",
    "    data_out = keras.layers.Conv1D(1, 1, name='conv1d_1')(data_out)\n",
    "    \n",
    "    model = Model(inputs=[data_input], outputs=[data_out])     \n",
    "    \n",
    " \n",
    "        \n",
    "    model.compile(loss='mae',\n",
    "                  optimizer=Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4e90e8e-d859-4908-b12b-75ee92605b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CWAFx(win_length, filters, kernel_size_1, learning_rate, wavenetConfig):\n",
    "    \n",
    "    kContext = 4 # past and subsequent frames\n",
    "    \n",
    "    x = Input(shape=(kContext*2+1, win_length, 1), name='input')\n",
    "    \n",
    "    conv = Conv1D(filters, kernel_size_1, strides=1, padding='same',\n",
    "                       kernel_initializer='lecun_uniform', input_shape=(win_length, 1))\n",
    "    \n",
    "    activation_abs = Activation(K.abs)\n",
    "    activation_sp = Activation('softplus')\n",
    "    max_pooling = MaxPooling1D(pool_size=win_length//64)\n",
    "\n",
    "    conv_smoothing = Conv1D_local(filters, kernel_size_1*2, strides=1, padding='same',\n",
    "                                  kernel_initializer='lecun_uniform')\n",
    "    \n",
    "   \n",
    "    deconv = Conv1D_tied(1, kernel_size_1, conv, padding='same', name='deconv')\n",
    "     \n",
    "        \n",
    "    X = TimeDistributed(conv, name='conv')(x)\n",
    "    X_abs = TimeDistributed(activation_abs, name='conv_activation')(X)\n",
    "    M = TimeDistributed(conv_smoothing, name='conv_smoothing')(X_abs)\n",
    "    M = TimeDistributed(activation_sp, name='conv_smoothing_activation')(M)\n",
    "    P = X\n",
    "    Z = TimeDistributed(max_pooling, name='max_pooling')(M)\n",
    "    Z = Lambda(lambda inputs: tf.unstack(inputs, num=kContext*2+1, axis=1, name='unstack2'))(Z)\n",
    "    Z = Concatenate(name='concatenate', axis=-2)(Z)\n",
    "    \n",
    "    Z = wavenet(Z, wavenetConfig, contextFrames=kContext, output_channels=filters, context=True)\n",
    "  \n",
    "    Z = Lambda((toPermuteDimensions), name='perm_1')(Z)\n",
    "    Z = Dense(win_length//64, activation = 'tanh', name = 'dense_wn')(Z)\n",
    "    Z = Lambda((toPermuteDimensions), name='perm_2')(Z)\n",
    "    \n",
    "    M_ = UpSampling1D(size=win_length//64, name='up_sampling_naive')(Z)\n",
    "    P = Lambda(lambda inputs: tf.unstack(inputs, num=kContext*2+1, axis=1, name='unstack'))(P)\n",
    "    Y = Multiply(name='phase_unpool_multiplication')([P[kContext],M_])\n",
    "\n",
    "    Y_ = Dense(filters, activation = 'tanh', name = 'dense_in')(Y)\n",
    "    Y_ = Dense(filters//2, activation = 'tanh', name = 'dense_h1')(Y_)   \n",
    "    Y_ = Dense(filters//2, activation = 'tanh', name = 'dense_h2')(Y_)\n",
    "    Y_ = Dense(filters, activation = 'linear', name = 'dense_out')(Y_)\n",
    " \n",
    "    Y_ = SAAF(break_points=25, break_range=0.2, magnitude=100, order=2, tied_feamap=True,\n",
    "            kernel_initializer = 'random_normal', name = 'saaf_out')(Y_)\n",
    "    \n",
    "    Y_ = se_block(Y_, filters, weight_decay=0., amplifying_ratio=16, idx = 1)\n",
    "    \n",
    "    Y = Add(name='addition')([Y,Y_])\n",
    "    \n",
    "    Y = deconv(Y)\n",
    "    \n",
    "    model = Model(inputs=[x], outputs=[Y])\n",
    "    \n",
    "    model.compile(loss={'deconv': 'mae'},\n",
    "                        loss_weights={'deconv': 1.0},\n",
    "                        optimizer=Adam(learning_rate=learning_rate))\n",
    "    \n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1972dd99-91b0-43ab-a091-b119da986baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "CWAFx_config = { 'epoch' : 2000,\n",
    "                    'filters' : 32,\n",
    "                    'kernelSize' : 64,\n",
    "                    'learningRate' : 0.0001,\n",
    "                    'winLength' : 4096,\n",
    "                    'modelsPath': './Models/',\n",
    "                    'monitorLoss': 'val_loss',\n",
    "                    'wavenetConfig': {\n",
    "                        'model': {\n",
    "                        'dilations': [1, 2, 4, 8, 16, 32, 64],\n",
    "                        'filters': {'depths': {'final': [32, 32],\n",
    "                        'res': 32,\n",
    "                        'skip': 32},\n",
    "                        'lengths': {'final': [3, 3], 'res': 3, 'skip': 1}},\n",
    "                        'input_length': 576,\n",
    "                        'num_stacks': 2,\n",
    "                        'target_field_length': 576,\n",
    "                        'target_padding': 0}}\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca04641c-4dff-42fc-a8f5-0636e1d547ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGen = GeneratorContext(Xtrain, Ytrain, kContext, CWAFx_config['winLength'], CWAFx_config['winLength']//2)\n",
    "valGen = GeneratorContext(Xval, Yval, kContext, CWAFx_config['winLength'], CWAFx_config['winLength']//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23846a8c-441b-4db2-af82-a4964c2e2fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    C:\\Users\\yavuz\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      " [py.warnings]\n"
     ]
    }
   ],
   "source": [
    "model_CWAFx = CWAFx(CWAFx_config['winLength'], \n",
    "                    CWAFx_config['filters'],  \n",
    "                    CWAFx_config['kernelSize'], \n",
    "                    CWAFx_config['learningRate'], \n",
    "                    CWAFx_config['wavenetConfig'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ece81-48df-4044-b117-db1e5c7ba058",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CWAFx.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ed23b18-8ad5-4463-b4d6-7d83df2b26d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    C:\\Users\\yavuz\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      " [py.warnings]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A total of 41 objects could not be loaded. Example error message for object <Conv1D name=conv1d, built=True>:\n\nLayer 'conv1d' expected 2 variables, but received 0 variables during loading. Expected: ['kernel', 'bias']\n\nList of objects that could not be loaded:\n[<Conv1D name=conv1d, built=True>, <Conv1D_local name=conv1d_local, built=True>, <Conv1D name=initial_causal_conv, built=True>, <Conv1D name=res_1_dilated_conv_d1_s0, built=True>, <Conv1D name=res_1_conv_d1_s0, built=True>, <Conv1D name=res_2_dilated_conv_d2_s0, built=True>, <Conv1D name=res_2_conv_d2_s0, built=True>, <Conv1D name=res_3_dilated_conv_d4_s0, built=True>, <Conv1D name=res_3_conv_d4_s0, built=True>, <Conv1D name=res_4_dilated_conv_d8_s0, built=True>, <Conv1D name=res_4_conv_d8_s0, built=True>, <Conv1D name=res_5_dilated_conv_d16_s0, built=True>, <Conv1D name=res_5_conv_d16_s0, built=True>, <Conv1D name=res_6_dilated_conv_d32_s0, built=True>, <Conv1D name=res_6_conv_d32_s0, built=True>, <Conv1D name=res_7_dilated_conv_d64_s0, built=True>, <Conv1D name=res_7_conv_d64_s0, built=True>, <Conv1D name=res_8_dilated_conv_d1_s1, built=True>, <Conv1D name=res_8_conv_d1_s1, built=True>, <Conv1D name=res_9_dilated_conv_d2_s1, built=True>, <Conv1D name=res_9_conv_d2_s1, built=True>, <Conv1D name=res_10_dilated_conv_d4_s1, built=True>, <Conv1D name=res_10_conv_d4_s1, built=True>, <Conv1D name=res_11_dilated_conv_d8_s1, built=True>, <Conv1D name=res_11_conv_d8_s1, built=True>, <Conv1D name=res_12_dilated_conv_d16_s1, built=True>, <Conv1D name=res_12_conv_d16_s1, built=True>, <Conv1D name=res_13_dilated_conv_d32_s1, built=True>, <Conv1D name=res_13_conv_d32_s1, built=True>, <Conv1D name=res_14_dilated_conv_d64_s1, built=True>, <Conv1D name=res_14_conv_d64_s1, built=True>, <Conv1D name=penultimate_conv_1d, built=True>, <Conv1D name=final_conv_1d, built=True>, <Dense name=dense_wn, built=True>, <Dense name=dense_in, built=True>, <Dense name=dense_h1, built=True>, <Dense name=dense_h2, built=True>, <Dense name=dense_out, built=True>, <SAAF name=saaf_out, built=True>, <Dense name=se_dense1_1, built=True>, <Dense name=se_dense2_1, built=True>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/yavuz/Musiccodes/MyCAFModels/Models/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model_CWAFx\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel_CWAFx_chk.weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:593\u001b[0m, in \u001b[0;36m_raise_loading_failure\u001b[1;34m(error_msgs, warn_only)\u001b[0m\n\u001b[0;32m    591\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg)\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: A total of 41 objects could not be loaded. Example error message for object <Conv1D name=conv1d, built=True>:\n\nLayer 'conv1d' expected 2 variables, but received 0 variables during loading. Expected: ['kernel', 'bias']\n\nList of objects that could not be loaded:\n[<Conv1D name=conv1d, built=True>, <Conv1D_local name=conv1d_local, built=True>, <Conv1D name=initial_causal_conv, built=True>, <Conv1D name=res_1_dilated_conv_d1_s0, built=True>, <Conv1D name=res_1_conv_d1_s0, built=True>, <Conv1D name=res_2_dilated_conv_d2_s0, built=True>, <Conv1D name=res_2_conv_d2_s0, built=True>, <Conv1D name=res_3_dilated_conv_d4_s0, built=True>, <Conv1D name=res_3_conv_d4_s0, built=True>, <Conv1D name=res_4_dilated_conv_d8_s0, built=True>, <Conv1D name=res_4_conv_d8_s0, built=True>, <Conv1D name=res_5_dilated_conv_d16_s0, built=True>, <Conv1D name=res_5_conv_d16_s0, built=True>, <Conv1D name=res_6_dilated_conv_d32_s0, built=True>, <Conv1D name=res_6_conv_d32_s0, built=True>, <Conv1D name=res_7_dilated_conv_d64_s0, built=True>, <Conv1D name=res_7_conv_d64_s0, built=True>, <Conv1D name=res_8_dilated_conv_d1_s1, built=True>, <Conv1D name=res_8_conv_d1_s1, built=True>, <Conv1D name=res_9_dilated_conv_d2_s1, built=True>, <Conv1D name=res_9_conv_d2_s1, built=True>, <Conv1D name=res_10_dilated_conv_d4_s1, built=True>, <Conv1D name=res_10_conv_d4_s1, built=True>, <Conv1D name=res_11_dilated_conv_d8_s1, built=True>, <Conv1D name=res_11_conv_d8_s1, built=True>, <Conv1D name=res_12_dilated_conv_d16_s1, built=True>, <Conv1D name=res_12_conv_d16_s1, built=True>, <Conv1D name=res_13_dilated_conv_d32_s1, built=True>, <Conv1D name=res_13_conv_d32_s1, built=True>, <Conv1D name=res_14_dilated_conv_d64_s1, built=True>, <Conv1D name=res_14_conv_d64_s1, built=True>, <Conv1D name=penultimate_conv_1d, built=True>, <Conv1D name=final_conv_1d, built=True>, <Dense name=dense_wn, built=True>, <Dense name=dense_in, built=True>, <Dense name=dense_h1, built=True>, <Dense name=dense_h2, built=True>, <Dense name=dense_out, built=True>, <SAAF name=saaf_out, built=True>, <Dense name=se_dense1_1, built=True>, <Dense name=se_dense2_1, built=True>]"
     ]
    }
   ],
   "source": [
    "os.chdir('C:/Users/yavuz/Musiccodes/MyCAFModels/Models/')\n",
    "model_CWAFx.load_weights('Model_CWAFx_chk.weights.h5');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b996a63-f44e-4628-aba7-2385e706cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CWAFx.load_weights(modelsPath+'Model_CWAFx' +'_chk.weights.h5', skip_mismatch=True) ;\n",
    "print ('CWAFx Pretraining finished.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176b7fa-1db2-4aed-8ebf-3d194bfb6c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CWAFx.load_weights(modelsPath + modelName+'.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7f733-1291-4e42-bbbb-f07da3f6c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/yavuz/Musiccodes/MyCAFModels/Models/')\n",
    "#model_CWAFx.load_weights('Model_CWAFx.weights.h5');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9c5487-45b0-46a0-9ff8-44fa10182174",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CWAFx.fit(trainGen,\n",
    "                steps_per_epoch=None,\n",
    "                epochs= 16,\n",
    "                verbose=2,\n",
    "                callbacks = [checkpointer, earlyStopping],\n",
    "                validation_data = valGen,\n",
    "                validation_steps=len(Xval),\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ee6b0-3f6c-48bd-9852-4f6e8bec1103",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/YavuzBURUKPEAKUP/Kods/MyCAFModels/Models/')\n",
    "model_CWAFx.save_weights('Test_A_L_A_L12_16K.weights.h5');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdffa78-a597-4c5d-877f-d37c9495e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_wav_file(filename, sampling_rate, audio_data):\n",
    "    # Ensure 16-bit integer format\n",
    "    if audio_data.dtype != np.int16:\n",
    "        audio_data = (np.clip(audio_data, -1.0, 1.0) * 32767).astype(np.int16)\n",
    "    \n",
    "    with wave.open(filename, 'wb') as wav_file:\n",
    "        wav_file.setnchannels(1 if len(audio_data.shape) == 1 else audio_data.shape[1])\n",
    "        wav_file.setsampwidth(2)  # 2 bytes = 16 bits\n",
    "        wav_file.setframerate(sampling_rate)\n",
    "        wav_file.writeframes(audio_data.tobytes())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2337d2-6aa4-41c7-a850-1d2a5c004c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wav_file(filename):\n",
    "    with wave.open(filename, 'rb') as wav_file:\n",
    "        # Get audio parameters\n",
    "        frames = wav_file.getnframes()\n",
    "        sample_rate = wav_file.getframerate()\n",
    "        channels = wav_file.getnchannels()\n",
    "        sample_width = wav_file.getsampwidth()\n",
    "        \n",
    "        # Read audio data\n",
    "        audio_data = wav_file.readframes(frames)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        if sample_width == 1:\n",
    "            dtype = np.uint8\n",
    "        elif sample_width == 2:\n",
    "            dtype = np.int16\n",
    "        elif sample_width == 4:\n",
    "            dtype = np.int32\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported sample width: {sample_width}\")\n",
    "        \n",
    "        audio_array = np.frombuffer(audio_data, dtype=dtype)\n",
    "        \n",
    "        # Reshape for stereo\n",
    "        if channels > 1:\n",
    "            audio_array = audio_array.reshape(-1, channels)\n",
    "        \n",
    "        return sample_rate, audio_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd65191-0fc3-4946-9afc-1adc453921b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b16e1-053e-4409-b121-0e937c1c63bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nofx_2_array_test = np.load(\"C:/Users/YavuzBURUKPEAKUP/Kods/MyCAFModels/data/grandhall_stereo_data_left.npy\")\n",
    "#reverb_2_array = np.load(\"C:/Users/YavuzBURUKPEAKUP/Kods/MyCAFModels/data/grandhall_atmos_data_SR.npy\")\n",
    "\n",
    "#nofx_2_array_test = nofx_2_array_test[0:data_length_include,:]\n",
    "#reverb_2_array = nofx_2_array[0:data_length_include,:]\n",
    "\n",
    "os.chdir('C:/Users/YavuzBURUKPEAKUP/Kods/MyCAFModels/Models/')\n",
    "model_CWAFx.load_weights('CH_Model_CWAFx_A_R_A_R_4_16K.weights.h5');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad5d43-dfde-4ad6-ba0e-fa01685332ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "nofx_2_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25951352-d021-456d-b041-fb2841fa70e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/yavuz/Musiccodes/MyCAFModels/Output/')\n",
    "\n",
    "print('CWAFx Evaluating ')\n",
    "metrics = {}\n",
    "mae = []\n",
    "mfcc_cosine = []\n",
    "mse_y = []\n",
    "mse_z = []\n",
    "\n",
    "main_idx = 5\n",
    "\n",
    "Xtest_pre = nofx_2_array[main_idx][0:length_in_samples]\n",
    "Ytest_pre = reverb_2_array[main_idx][0:length_in_samples]\n",
    "\n",
    "Xtest = Xtest_pre.reshape(1,length_in_samples,1)\n",
    "Ytest = Ytest_pre.reshape(1,length_in_samples,1)\n",
    "\n",
    "Xtest = utils.cropAndPad(Xtest, crop = 0, pad = kContext*winLength//2)\n",
    "Ytest = utils.cropAndPad(Ytest, crop = 0, pad = kContext*winLength//2)\n",
    "\n",
    "kLen = Xtest.shape[1]\n",
    "kBatch = int((kLen/(winLength//2)) + 1)\n",
    "\n",
    "testGen = GeneratorContext(Xtest, Ytest, kContext, winLength, winLength//2)\n",
    "\n",
    "for idx in range(Xtest.shape[0]):\n",
    "        x = testGen[idx][0]\n",
    "        Z = model_CWAFx.predict(x, batch_size=kBatch)\n",
    "        Z_m = Z[:,:,0]\n",
    "        Ztest_waveform = overlap(Z_m, kLen, winLength, winLength//2, windowing=True, rate=2)\n",
    "        Ytest_waveform = Ytest[idx].reshape(Ytest[idx].shape[0])\n",
    "        audio_data = (Ztest_waveform * 32767).astype(np.int16)\n",
    "        write_wav_file('Waveform12.wav', 16000, audio_data)\n",
    "\n",
    "       \n",
    "        mae.append(utils.getMAEnormalized(Ytest_waveform, Ztest_waveform))\n",
    "        d = utils.getMSE_MFCC(Ytest_waveform, Ztest_waveform, kSR, mean_norm=False)    \n",
    "        mfcc_cosine.append(d['cosine'])\n",
    "        ms, e_y, _ = getMP(Ytest_waveform, kLen)\n",
    "        ms, e_z, _ = getMP(Ztest_waveform, kLen)\n",
    "        mse_y.append(e_y)\n",
    "        mse_z.append(e_z)\n",
    "\n",
    "d = utils.getDistances(np.asarray(mse_y), np.asarray(mse_z))    \n",
    "metrics['mae'] = round(np.mean(mae), 5)\n",
    "metrics['mfcc_cosine'] = round(np.mean(mfcc_cosine), 5)\n",
    "metrics['msed'] = round(np.mean(d['euclidean']), 5)\n",
    "    \n",
    "for metric in metrics.items():\n",
    "    print(metric)\n",
    "\n",
    "print('Evaluation finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6b825-26b6-4bed-bdce-d3bf2daaf44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_idxs = np.arange(0, 104)\n",
    "print(len(main_idxs))\n",
    "main_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a3b6c-0531-46c5-8d05-c68fe161ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/yavuz/Musiccodes/MyCAFModels/ModelsLast/')\n",
    "\n",
    "my_models = ['Model_C_16.weights.h5', 'Model_L_16.weights.h5', 'Model_L_L_16.weights.h5', 'Model_L_RSL_16.weights.h5',\n",
    "             'Model_L_SL_16.weights.h5', 'Model_L_TFL_16.weights.h5', 'Model_R_16.weights.h5', 'Model_R_R_16.weights.h5',\n",
    "             'Model_R_RSR_16.weights.h5', 'Model_R_SR_16.weights.h5', 'Model_R_TFR_16.weights.h5', 'Model_RSL_16.weights.h5',\n",
    "             'Model_RSR_16.weights.h5', 'Model_SL_16.weights.h5', 'Model_SR_16.weights.h5', 'Model_TFL_16.weights.h5',\n",
    "             'Model_TFR_16.weights.h5']\n",
    "\n",
    "model_idx = 2\n",
    "model_to_test = my_models[model_idx]\n",
    "print(\"Model: \", model_to_test)\n",
    "model_CWAFx.load_weights(model_to_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a1d66-b673-4b28-bf22-5ff4f1fcc8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/yavuz/Musiccodes/MyCAFModels/data/\")\n",
    "\n",
    "nofx_2_array = np.load(\"grandhall_stereo_data_left.npy\")\n",
    "print(\"Training Array Shape: \", nofx_2_array.shape)\n",
    "\n",
    "reverb_2_array = np.load(\"grandhall_atmos_data_left.npy\")\n",
    "print(\"Output Array Shape\", reverb_2_array.shape)\n",
    "\n",
    "nofx_2_array_test_16 = np.zeros((104, 64000), dtype=np.float32)\n",
    "reverb_2_array_16 = np.zeros((104, 64000), dtype=np.float32)\n",
    "\n",
    "for i in range(104):\n",
    "    if (i%20==0):\n",
    "        print(i)\n",
    "    nofx_2_array_16[i,:] = resampy.resample(nofx_2_array[i,:], sr_orig=48000, sr_new=16000)\n",
    "    reverb_2_array_16[i,:] = resampy.resample(reverb_2_array[i,:], sr_orig=48000, sr_new=16000)\n",
    "    \n",
    "\n",
    "print(nofx_2_array_16.shape, reverb_2_array_16.shape)\n",
    "\n",
    "nofx_2_array = nofx_2_array_16\n",
    "reverb_2_array = reverb_2_array_16\n",
    "\n",
    "print(nofx_2_array.shape, reverb_2_array.shape)\n",
    "\n",
    "data_length_include = 104\n",
    "\n",
    "nofx_2_array = nofx_2_array[0:data_length_include,:]\n",
    "print(\"Train\", nofx_2_array.shape)\n",
    "\n",
    "reverb_2_array = reverb_2_array[0:data_length_include,:]\n",
    "print(\"Output\", reverb_2_array.shape)\n",
    "\n",
    "length_in_samples = nofx_2_array.shape[1]\n",
    "\n",
    "print(\"Length in samples\", length_in_samples)\n",
    "\n",
    "nofx_2_array_reshaped = nofx_2_array.reshape(data_length_include, length_in_samples, 1)\n",
    "reverb_2_array_reshaped = reverb_2_array.reshape(data_length_include, length_in_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03fc85c-79ea-4bc9-82da-6ce530d7ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/yavuz/Musiccodes/MyCAFModels/Output/')\n",
    "\n",
    "print('CWAFx Evaluating ')\n",
    "metrics = {}\n",
    "mae_s = []\n",
    "mfcc_cosine_s = []\n",
    "msed_s = []\n",
    "mse_y = []\n",
    "mse_z = []\n",
    "\n",
    "for main_idx in main_idxs:\n",
    "    print(\"main_idx\", main_idx)\n",
    "    #main_idx = 5\n",
    "    mae = []\n",
    "    mfcc_cosine = []\n",
    "    mse_y = []\n",
    "    mse_z = []\n",
    "\n",
    "\n",
    "    \n",
    "    Xtest_pre = nofx_2_array[main_idx][0:length_in_samples];\n",
    "    Ytest_pre = reverb_2_array[main_idx][0:length_in_samples];\n",
    "    \n",
    "    Xtest = Xtest_pre.reshape(1,length_in_samples,1);\n",
    "    Ytest = Ytest_pre.reshape(1,length_in_samples,1);\n",
    "    \n",
    "    Xtest = utils.cropAndPad(Xtest, crop = 0, pad = kContext*winLength//2);\n",
    "    Ytest = utils.cropAndPad(Ytest, crop = 0, pad = kContext*winLength//2);\n",
    "    \n",
    "    kLen = Xtest.shape[1];\n",
    "    kBatch = int((kLen/(winLength//2)) + 1);\n",
    "    \n",
    "    testGen = GeneratorContext(Xtest, Ytest, kContext, winLength, winLength//2);\n",
    "    \n",
    "    for idx in range(Xtest.shape[0]):\n",
    "            x = testGen[idx][0];\n",
    "            Z = model_CWAFx.predict(x, batch_size=kBatch);\n",
    "            Z_m = Z[:,:,0];\n",
    "            Ztest_waveform = overlap(Z_m, kLen, winLength, winLength//2, windowing=True, rate=2);\n",
    "            Ytest_waveform = Ytest[idx].reshape(Ytest[idx].shape[0]);\n",
    "            #audio_data = (Ztest_waveform * 32767).astype(np.int16);\n",
    "            #write_wav_file('Waveform' + str(main_idx) + '.wav', 16000, audio_data);\n",
    "    \n",
    "           \n",
    "            mae.append(utils.getMAEnormalized(Ytest_waveform, Ztest_waveform));\n",
    "            d = utils.getMSE_MFCC(Ytest_waveform, Ztest_waveform, kSR, mean_norm=False);    \n",
    "            mfcc_cosine.append(d['cosine']);\n",
    "            ms, e_y, _ = getMP(Ytest_waveform, kLen);\n",
    "            ms, e_z, _ = getMP(Ztest_waveform, kLen);\n",
    "            mse_y.append(e_y);\n",
    "            mse_z.append(e_z);\n",
    "    \n",
    "    d = utils.getDistances(np.asarray(mse_y), np.asarray(mse_z));    \n",
    "    #print('MAE', round(np.mean(mae), 5))\n",
    "    #print(\"MCOSINE\", round(np.mean(mfcc_cosine), 5))\n",
    "    #print(\"MSD\", round(np.mean(d['euclidean']), 5))\n",
    "    mae_s.append(round(np.mean(mae), 5))\n",
    "    mfcc_cosine_s.append(round(np.mean(mfcc_cosine), 5))\n",
    "    msed_s.append(round(np.mean(d['euclidean']), 5))\n",
    "    metrics['mae'] = round(np.mean(mae), 5);\n",
    "    metrics['mfcc_cosine'] = round(np.mean(mfcc_cosine), 5);\n",
    "    metrics['msed'] = round(np.mean(d['euclidean']), 5);\n",
    "        \n",
    "    #for metric in metrics.items():\n",
    "    #    print(metric)\n",
    "    \n",
    "print('Evaluation finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a8168d-863e-431a-9a20-d52a061ef80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(mae_s))\n",
    "print(np.median(mae_s))\n",
    "print(np.mean(mfcc_cosine_s))\n",
    "print(np.median(mfcc_cosine_s))\n",
    "print(np.mean(msed_s))\n",
    "print(np.median(msed_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4f779-d087-4a08-bb48-1f266ab01829",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mae_s);\n",
    "print(np.mean(mae_s))\n",
    "print(np.median(mae_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a64ad06-f208-4a99-bae6-ae9c8a28d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mfcc_cosine_s)\n",
    "print(np.mean(mfcc_cosine_s))\n",
    "print(np.median(mfcc_cosine_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b973e6-1e88-4e19-a1ac-7390e8f8c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(msed_s)\n",
    "print(np.mean(msed_s))\n",
    "print(np.median(msed_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133dcbb0-972b-4dc4-b4d3-fcef7c3bf66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e73470-58aa-4c00-a063-1fa75f4e0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.linspace(0, 4, length_in_samples)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(3, 1, 1) \n",
    "plt.plot(time, Xtest_pre/max(Xtest_pre));\n",
    "plt.title(\"Input Test Signal\", fontsize=10)\n",
    "\n",
    "\n",
    "plt.subplot(3, 1, 2) \n",
    "plt.plot(time, Ytest_pre/max(Ytest_pre));\n",
    "plt.title(\"Output Test Signal\", fontsize=10);\n",
    "\n",
    "sampling_rate, audio_data = read_wav_file('Waveform12.wav')\n",
    "\n",
    "plt.subplot(3, 1, 3) \n",
    "plt.plot(time, audio_data[0:length_in_samples]/max(audio_data[0:length_in_samples]))\n",
    "plt.xlabel(\"Time in secs\")\n",
    "plt.title(\"Output Test Signal of CWAFx\", fontsize=10);\n",
    "\n",
    "plt.savefig('DatabaseSample12.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "write_wav_file(\"Waveform12_In.wav\", 16000, Xtest_pre/max(Xtest_pre))\n",
    "write_wav_file(\"Waveform12_Out.wav\", 16000, Ytest_pre/max(Ytest_pre))\n",
    "write_wav_file(\"Waveform12_Model.wav\", 16000, audio_data[0:length_in_samples]/max(audio_data[0:length_in_samples]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3901fa-8437-4ce2-909e-9fb440a1eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input - Training\")\n",
    "Audio(data=Xtest_pre.T, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efea3d42-84b7-4257-acdc-8bce1d56c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output\")\n",
    "Audio(data=Ytest_pre.T, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481e73bf-e140-49a3-a403-5216719fe1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output of Model\")\n",
    "Audio(data=audio_data, rate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efceb9b5-6e9a-4537-91fa-1c9ca31555fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/yavuz/Musiccodes/MyCAFModels/Output/')\n",
    "\n",
    "print('CWAFx Evaluating ')\n",
    "\n",
    "main_idx = 12\n",
    "\n",
    "\n",
    "Xtest_pre = nofx_2_array_test[main_idx][0:length_in_samples] + nofx_2_array_test[main_idx+10][0:length_in_samples] + nofx_2_array_test[main_idx+20][0:length_in_samples] + nofx_2_array_test[main_idx+30][0:length_in_samples]\n",
    "Ytest_pre = reverb_2_array[main_idx][0:length_in_samples] + reverb_2_array[main_idx+10][0:length_in_samples] + reverb_2_array[main_idx+20][0:length_in_samples] + reverb_2_array[main_idx+30][0:length_in_samples]\n",
    "\n",
    "\n",
    "Xtest = Xtest_pre.reshape(1,length_in_samples,1)\n",
    "Ytest = Ytest_pre.reshape(1,length_in_samples,1)\n",
    "#print(\"Xtest: \", Xtest.shape, \" Ytest: \", Ytest.shape)\n",
    "\n",
    "# zero pad at the end as well. \n",
    "Xtest = utils.cropAndPad(Xtest, crop = 0, pad = kContext*winLength//2)\n",
    "Ytest = utils.cropAndPad(Ytest, crop = 0, pad = kContext*winLength//2)\n",
    "\n",
    "\n",
    "kLen = Xtest.shape[1]\n",
    "kBatch = int((kLen/(winLength//2)) + 1)\n",
    "\n",
    "testGen = GeneratorContext(Xtest, Ytest, kContext, winLength, winLength//2)\n",
    "metrics = {}\n",
    "mae = []\n",
    "mfcc_cosine = []\n",
    "mse_y = []\n",
    "mse_z = []\n",
    "\n",
    "#model.load_weights(\"C:\\Users\\yavuz\\Musiccodes\\MyCAFModels\\Models\\Model1.weights.h5\", by_name=True) \n",
    "\n",
    "for idx in range(Xtest.shape[0]):\n",
    "        print(idx)\n",
    "\n",
    "        x = testGen[idx][0]\n",
    "        Z = model_CWAFx.predict(x, batch_size=kBatch)\n",
    "        Z_m = Z[:,:,0]\n",
    "        Ztest_waveform = overlap(Z_m, kLen, winLength, winLength//2, windowing=True, rate=2)\n",
    "        \n",
    "        Ytest_waveform = Ytest[idx].reshape(Ytest[idx].shape[0])\n",
    "\n",
    "        # Convert to 16-bit integers\n",
    "        audio_data = (Ztest_waveform * 32767).astype(np.int16)\n",
    "        \n",
    "        wavfile.write('Waveform12_22_32_42.wav', 16000, audio_data)\n",
    "        mae.append(utils.getMAEnormalized(Ytest_waveform, Ztest_waveform))\n",
    "        d = utils.getMSE_MFCC(Ytest_waveform, Ztest_waveform, kSR, mean_norm=False)    \n",
    "        mfcc_cosine.append(d['cosine'])\n",
    "        ms, e_y, _ = getMP(Ytest_waveform, kLen)\n",
    "        ms, e_z, _ = getMP(Ztest_waveform, kLen)\n",
    "        mse_y.append(e_y)\n",
    "        mse_z.append(e_z)\n",
    "\n",
    "#d = utils.getDistances(np.asarray(mse_y), np.asarray(mse_z))    \n",
    "metrics['mae'] = round(np.mean(mae), 5)\n",
    "metrics['mfcc_cosine'] = round(np.mean(mfcc_cosine), 5)\n",
    "metrics['msed'] = round(np.mean(d['euclidean']), 5)\n",
    "    \n",
    "for metric in metrics.items():\n",
    "    print(metric)\n",
    "        \n",
    "#with open('./' + 'CWAFx_metrics.json', 'w') as outfile:\n",
    "#    json.dump(metrics, outfile)\n",
    "\n",
    "print(\"MSE_y\", mse_y)\n",
    " \n",
    "print('Evaluation finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2beb596-f9ef-4696-8e39-8ff8d2dc8c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.linspace(0, 4, 16000*4)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(3, 1, 1) \n",
    "plt.plot(time, Xtest_pre/max(Xtest_pre));\n",
    "plt.title(\"Input Test Signal\", fontsize=10)\n",
    "\n",
    "\n",
    "plt.subplot(3, 1, 2) \n",
    "plt.plot(time, Ytest_pre/max(Ytest_pre));\n",
    "plt.title(\"Output Test Signal\", fontsize=10);\n",
    "\n",
    "\n",
    "sampling_rate, audio_data = read_wav_file('Waveform12_22_32_42.wav')\n",
    "\n",
    "plt.subplot(3, 1, 3) \n",
    "plt.plot(time, audio_data[0:length_in_samples]/max(audio_data[0:length_in_samples]))\n",
    "plt.xlabel(\"Time in secs\")\n",
    "plt.title(\"Output Test Signal of CWAFx\", fontsize=10);\n",
    "\n",
    "plt.savefig('DatabaseSample12_22_32_42.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "write_wav_file(\"Waveform12_22_32_42_In.wav\", kSR, Xtest_pre/max(Xtest_pre))\n",
    "write_wav_file(\"Waveform12_22_32_42_Out.wav\", kSR, Ytest_pre/max(Ytest_pre))\n",
    "write_wav_file(\"Waveform12_22_32_42_Model.wav\", kSR , audio_data[0:192000]/max(audio_data[0:192000]))\n",
    "\n",
    "\n",
    "write_wav_file(\"Waveform12_In.wav\", 16000, Xtest_pre/max(Xtest_pre))\n",
    "write_wav_file(\"Waveform12_Out.wav\", 16000, Ytest_pre/max(Ytest_pre))\n",
    "write_wav_file(\"Waveform12_Model.wav\", 16000, audio_data[0:length_in_samples]/max(audio_data[0:length_in_samples]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fbe07f-9f7d-4d81-8cb6-b82ad124bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NoFX\")\n",
    "Audio(data=Xtest_pre.T, rate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d39c8b3-332b-4542-95c4-6ccb8d580975",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With Reverb\")\n",
    "Audio(data=Ytest_pre.T, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b87016-33b7-48d7-a637-76bb619d1b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output of Model\")\n",
    "Audio(data=audio_data, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d076c360-2312-4350-bc36-0f4cac030600",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_length = 24\n",
    "\n",
    "with wave.open('C:/Users/yavuz/Musiccodes/MyCAFModels/data/DT1mono.wav', 'rb') as wav_file:\n",
    "    # Extract parameters\n",
    "    n_channels = wav_file.getnchannels()\n",
    "    sampwidth = wav_file.getsampwidth()\n",
    "    sampling_rate = wav_file.getframerate()\n",
    "    n_frames = wav_file.getnframes()\n",
    "\n",
    "    # Read the frames\n",
    "    frames = wav_file.readframes(n_frames)\n",
    "\n",
    "# Convert the frame data to an array of integers\n",
    "audio_data = np.frombuffer(frames, dtype=np.int16)\n",
    "\n",
    "# If stereo, split channels\n",
    "if n_channels == 2:\n",
    "    audio_data = audio_data.reshape(-1, 2)\n",
    "\n",
    "audio_data_s = audio_data[0:audio_length*sampling_rate]\n",
    "audio_data_s = audio_data_s / 32267\n",
    "\n",
    "print(\"Sampling Rate\", sampling_rate)\n",
    "\n",
    "time = np.linspace(0, audio_length, audio_length * sampling_rate)\n",
    "print(\"Sampling rate: \", sampling_rate)\n",
    "print(\"Length : \", n_frames/sampling_rate, \"seconds\")\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(time, audio_data_s);\n",
    "plt.xlabel(\"Time in secs\")\n",
    "Audio(data=audio_data_s, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5bf18-736f-4476-8e49-7987a40ca913",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_duration = 2\n",
    "audio_data_s_resampled = resampy.resample(audio_data_s, sr_orig=44100, sr_new=48000)\n",
    "number_of_frames = int(len(audio_data_s_resampled)/(frame_duration*48000))\n",
    "print(\"Number of Frames\", number_of_frames)\n",
    "audio_data_s_2d = audio_data_s_resampled.reshape(number_of_frames, frame_duration*48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51377402-2ace-43af-b527-3a9aa3927fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data_s_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c6049-8b24-41e2-9486-29817a977d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_10_down = resampy.resample(audio_data_s_2d[10,:], sr_orig=48000, sr_new=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3647c9bf-097e-4413-b184-db099e5dc5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(frame_10_down);\n",
    "Audio(data=frame_10_down, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5756afd-f886-485a-a76d-232dc30a4832",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_10_down_up = resampy.resample(frame_10_down, sr_orig=16000, sr_new=48000)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(frame_10_down_up);\n",
    "Audio(data=frame_10_down_up, rate=48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60562da4-ad2e-4f33-b8bd-7e3568969352",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = np.zeros(32000)\n",
    "for part_idx in range(0,number_of_frames):\n",
    "    print(part_idx)\n",
    "    Xtest_pre = audio_data_s_2d[part_idx]\n",
    "    #print(\"Xtest_pre\", Xtest_pre.shape)\n",
    "    frame_down = resampy.resample(Xtest_pre, sr_orig=48000, sr_new=16000)\n",
    "    \n",
    "    Xtest_pre_padded = np.concatenate([frame_down, zeros])\n",
    "    \n",
    "    Xtest = Xtest_pre_padded.reshape(1,64000,1)\n",
    "    Xtest = utils.cropAndPad(Xtest, crop = 0, pad = kContext*winLength//2)    # zero pad at the end as well. \n",
    "\n",
    "    kLen = Xtest.shape[1]\n",
    "    kBatch = int((kLen/(winLength//2)) + 1)\n",
    "\n",
    "    testGen = GeneratorContext(Xtest, Ytest, kContext, winLength, winLength//2)\n",
    "\n",
    "    for idx in range(Xtest.shape[0]):\n",
    "\n",
    "\n",
    "        x = testGen[idx][0]\n",
    "        Z = model_CWAFx.predict(x, batch_size=kBatch)\n",
    "        Z_m = Z[:,:,0]\n",
    "        Ztest_waveform = overlap(Z_m, kLen, winLength, winLength//2, windowing=True, rate=2)\n",
    "    \n",
    "\n",
    "    frame_down_up = resampy.resample(Ztest_waveform, sr_orig=16000, sr_new=48000)\n",
    "    print(\"Frame Down Up\", frame_down_up.shape)\n",
    "    \n",
    "    write_wav_file(\"CWAFx\"+'_'+str(part_idx)+'.wav', 48000, frame_down_up)\n",
    "    \n",
    "    #sf.write(\"CWAFx\"+'_'+str(part_idx)+'.wav', Ztest_waveform, kSR)\n",
    " \n",
    " \n",
    "print('Evaluation finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4aa02f-8bc4-41ef-b9d8-30a963100f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_reverb= np.zeros(number_of_frames*96000 + 96000)\n",
    "\n",
    "for i in range(0,number_of_frames):\n",
    "    sampling_rate, audio_data = read_wav_file('CWAFx_' + str(i) + '.wav')\n",
    "    #print(\"Sampling_rate\", sampling_rate)\n",
    "    #print(\"Audio_data\", audio_data.shape)\n",
    "    input_reverb[i*96000:i*96000+192000] = audio_data[0:192000]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(time , input_reverb[0:len(time)]);\n",
    "plt.title(\"Output Signal of CWAFx\")\n",
    "plt.savefig('DreamTheater.png', dpi=300, bbox_inches='tight');\n",
    "Audio(data=input_reverb, rate=48000)\n",
    "sf.write(\"DreamTheater.wav\", input_reverb[0:1152000]/max(input_reverb[0:1152000]), kSR)\n",
    "\n",
    "Audio(data=input_reverb, rate=sampling_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54b989-c524-4cc0-91a5-0c8682ca3041",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(time , input_reverb[0:len(time)]);\n",
    "plt.title(\"Output Signal of CWAFx\")\n",
    "plt.savefig('DreamTheater.png', dpi=300, bbox_inches='tight');\n",
    "Audio(data=input_reverb, rate=48000)\n",
    "sf.write(\"DreamTheater.wav\", input_reverb[0:1152000]/max(input_reverb[0:1152000]), kSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f09b3d7-3cf9-4725-a360-2152b91a138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.arange(0, 24, 1 / 22050)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(tt,input_reverb);\n",
    "plt.title(\"DreamTheater with reverberation using CWAFx\") # \"JohnMcLaughlin-AlDiMeola-PacoPena\" \"PearlJam\" \"DreamTheater\"\n",
    "sf.write(\"WaveformModel.wav\", audio_data[0:192000]/max(audio_data[0:192000]), kSR)\n",
    "Audio(data=input_reverb, rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432d80d2-c8b5-4d0e-ade5-86276d5d159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.write(\"CWAFx\"+'Song' + '.wav', input_reverb, 22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bceda83-a83c-4958-a1db-70dc3bcb11dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3773de27-8c15-4f7c-9970-60da055ef4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/YavuzBURUKPEAKUP/Kods/MyCAFModels/Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c76157-7da7-44e2-b72f-647528ed7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c0ce52-1004-4e16-981d-45716cd43326",
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06222787-995a-4615-a080-d3bcecc67386",
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf3a928-003a-4d1f-bc71-62930d5c170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(models[i][12:-12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01e8dae-ccc8-434b-baae-a6ef74cf2fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
